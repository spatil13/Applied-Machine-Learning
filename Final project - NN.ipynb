{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final project\n",
    "1. Build a neural network model for a very simple classification task on the data provided here.\n",
    "2. We are using Tensor flow lib."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Find the simplest neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import required libs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read data from file(data set)\n",
    "Data = pd.read_csv('input_data.csv') \n",
    "#separate the output attribute as label for training puropse\n",
    "output = Data['Label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert label into numerical format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### our output label is in the form of char (r,g,b) but in arctitecture we can use numeric valuse so we create label in interger form as\n",
    "    r = 0\n",
    "    g = 1\n",
    "    b = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "label= []\n",
    "for value in output:\n",
    "    if value=='r':\n",
    "        label.append(0)\n",
    "    elif value=='g':\n",
    "        label.append(1)\n",
    "    elif value=='b':\n",
    "        label.append(2)\n",
    "label = np.array(label)        \n",
    "data = Data.drop('Label',axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### spliting the data set 80% for training the model and 20% for the testing and evaluating the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(548,)\n"
     ]
    }
   ],
   "source": [
    "trainingData, testingData, trainingLabel, testingLabel = train_test_split(data, label, test_size=0.2)\n",
    "print(trainingLabel.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multilayer architecure \n",
    "1. Total 4 layers use in first layer 64 neurons, in 2nd layer 128, in 3rd 256 and in last(output) layer 3 Neurons used.\n",
    "2. Used relu as activation function in all layers except last layer. In last layer use softmax as activation function\n",
    "3. Sparse softmax cross entropy is used for loss calculation\n",
    "4. Adam optimizer is used with learning rate of 0.0001 for avoiding overfiting and better learning  use dropout(0.5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/sapatil/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /Users/sapatil/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /Users/sapatil/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/losses/losses_impl.py:209: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 548 samples, validate on 137 samples\n",
      "Epoch 1/40\n",
      "548/548 [==============================] - 1s 2ms/sample - loss: 0.8646 - acc: 0.7664 - val_loss: 0.7076 - val_acc: 1.0000\n",
      "Epoch 2/40\n",
      "548/548 [==============================] - 0s 179us/sample - loss: 0.7062 - acc: 0.9690 - val_loss: 0.6945 - val_acc: 1.0000\n",
      "Epoch 3/40\n",
      "548/548 [==============================] - 0s 170us/sample - loss: 0.6830 - acc: 0.9982 - val_loss: 0.6935 - val_acc: 1.0000\n",
      "Epoch 4/40\n",
      "548/548 [==============================] - 0s 177us/sample - loss: 0.6846 - acc: 0.9945 - val_loss: 0.6934 - val_acc: 1.0000\n",
      "Epoch 5/40\n",
      "548/548 [==============================] - 0s 174us/sample - loss: 0.6789 - acc: 1.0000 - val_loss: 0.6931 - val_acc: 1.0000\n",
      "Epoch 6/40\n",
      "548/548 [==============================] - 0s 176us/sample - loss: 0.6803 - acc: 0.9982 - val_loss: 0.6929 - val_acc: 1.0000\n",
      "Epoch 7/40\n",
      "548/548 [==============================] - 0s 173us/sample - loss: 0.6784 - acc: 1.0000 - val_loss: 0.6928 - val_acc: 1.0000\n",
      "Epoch 8/40\n",
      "548/548 [==============================] - 0s 175us/sample - loss: 0.6787 - acc: 1.0000 - val_loss: 0.6926 - val_acc: 1.0000\n",
      "Epoch 9/40\n",
      "548/548 [==============================] - 0s 179us/sample - loss: 0.6779 - acc: 1.0000 - val_loss: 0.6926 - val_acc: 1.0000\n",
      "Epoch 10/40\n",
      "548/548 [==============================] - 0s 189us/sample - loss: 0.6771 - acc: 0.9982 - val_loss: 0.6924 - val_acc: 1.0000\n",
      "Epoch 11/40\n",
      "548/548 [==============================] - 0s 188us/sample - loss: 0.6772 - acc: 0.9982 - val_loss: 0.6920 - val_acc: 1.0000\n",
      "Epoch 12/40\n",
      "548/548 [==============================] - 0s 166us/sample - loss: 0.6766 - acc: 1.0000 - val_loss: 0.6915 - val_acc: 1.0000\n",
      "Epoch 13/40\n",
      "548/548 [==============================] - 0s 185us/sample - loss: 0.6785 - acc: 0.9982 - val_loss: 0.6914 - val_acc: 1.0000\n",
      "Epoch 14/40\n",
      "548/548 [==============================] - 0s 189us/sample - loss: 0.6773 - acc: 1.0000 - val_loss: 0.6911 - val_acc: 1.0000\n",
      "Epoch 15/40\n",
      "548/548 [==============================] - 0s 179us/sample - loss: 0.6767 - acc: 1.0000 - val_loss: 0.6908 - val_acc: 1.0000\n",
      "Epoch 16/40\n",
      "548/548 [==============================] - 0s 429us/sample - loss: 0.6754 - acc: 1.0000 - val_loss: 0.6905 - val_acc: 1.0000\n",
      "Epoch 17/40\n",
      "548/548 [==============================] - 0s 175us/sample - loss: 0.6746 - acc: 0.9982 - val_loss: 0.6898 - val_acc: 1.0000\n",
      "Epoch 18/40\n",
      "548/548 [==============================] - 0s 173us/sample - loss: 0.6764 - acc: 1.0000 - val_loss: 0.6895 - val_acc: 1.0000\n",
      "Epoch 19/40\n",
      "548/548 [==============================] - 0s 208us/sample - loss: 0.6761 - acc: 1.0000 - val_loss: 0.6892 - val_acc: 1.0000\n",
      "Epoch 20/40\n",
      "548/548 [==============================] - 0s 176us/sample - loss: 0.6744 - acc: 1.0000 - val_loss: 0.6888 - val_acc: 1.0000\n",
      "Epoch 21/40\n",
      "548/548 [==============================] - 0s 187us/sample - loss: 0.6747 - acc: 1.0000 - val_loss: 0.6884 - val_acc: 1.0000\n",
      "Epoch 22/40\n",
      "548/548 [==============================] - 0s 177us/sample - loss: 0.6732 - acc: 1.0000 - val_loss: 0.6881 - val_acc: 1.0000\n",
      "Epoch 23/40\n",
      "548/548 [==============================] - 0s 172us/sample - loss: 0.6758 - acc: 1.0000 - val_loss: 0.6878 - val_acc: 1.0000\n",
      "Epoch 24/40\n",
      "548/548 [==============================] - 0s 319us/sample - loss: 0.6721 - acc: 0.9982 - val_loss: 0.6876 - val_acc: 1.0000\n",
      "Epoch 25/40\n",
      "548/548 [==============================] - 0s 202us/sample - loss: 0.6738 - acc: 0.9982 - val_loss: 0.6873 - val_acc: 1.0000\n",
      "Epoch 26/40\n",
      "548/548 [==============================] - 0s 201us/sample - loss: 0.6727 - acc: 0.9982 - val_loss: 0.6871 - val_acc: 1.0000\n",
      "Epoch 27/40\n",
      "548/548 [==============================] - 0s 210us/sample - loss: 0.6733 - acc: 0.9982 - val_loss: 0.6869 - val_acc: 1.0000\n",
      "Epoch 28/40\n",
      "548/548 [==============================] - 0s 196us/sample - loss: 0.6733 - acc: 0.9982 - val_loss: 0.6867 - val_acc: 1.0000\n",
      "Epoch 29/40\n",
      "548/548 [==============================] - 0s 204us/sample - loss: 0.6728 - acc: 1.0000 - val_loss: 0.6864 - val_acc: 1.0000\n",
      "Epoch 30/40\n",
      "548/548 [==============================] - 0s 191us/sample - loss: 0.6730 - acc: 0.9982 - val_loss: 0.6862 - val_acc: 0.9927\n",
      "Epoch 31/40\n",
      "548/548 [==============================] - 0s 225us/sample - loss: 0.6735 - acc: 1.0000 - val_loss: 0.6860 - val_acc: 0.9854\n",
      "Epoch 32/40\n",
      "548/548 [==============================] - 0s 192us/sample - loss: 0.6722 - acc: 0.9945 - val_loss: 0.6858 - val_acc: 0.9854\n",
      "Epoch 33/40\n",
      "548/548 [==============================] - 0s 165us/sample - loss: 0.6726 - acc: 0.9982 - val_loss: 0.6854 - val_acc: 0.9854\n",
      "Epoch 34/40\n",
      "548/548 [==============================] - 0s 387us/sample - loss: 0.6714 - acc: 0.9964 - val_loss: 0.6852 - val_acc: 0.9854\n",
      "Epoch 35/40\n",
      "548/548 [==============================] - 0s 185us/sample - loss: 0.6701 - acc: 0.9891 - val_loss: 0.6850 - val_acc: 0.9781\n",
      "Epoch 36/40\n",
      "548/548 [==============================] - 0s 173us/sample - loss: 0.6688 - acc: 0.9909 - val_loss: 0.6851 - val_acc: 0.9708\n",
      "Epoch 37/40\n",
      "548/548 [==============================] - 0s 174us/sample - loss: 0.6702 - acc: 0.9945 - val_loss: 0.6850 - val_acc: 0.9708\n",
      "Epoch 38/40\n",
      "548/548 [==============================] - 0s 170us/sample - loss: 0.6688 - acc: 0.9891 - val_loss: 0.6845 - val_acc: 0.9708\n",
      "Epoch 39/40\n",
      "548/548 [==============================] - 0s 165us/sample - loss: 0.6692 - acc: 0.9927 - val_loss: 0.6850 - val_acc: 0.9708\n",
      "Epoch 40/40\n",
      "548/548 [==============================] - 0s 168us/sample - loss: 0.6685 - acc: 0.9891 - val_loss: 0.6847 - val_acc: 0.9635\n"
     ]
    }
   ],
   "source": [
    "epochs=40\n",
    "model = tf.keras.Sequential();\n",
    "model.add(tf.keras.layers.Dense(units=64, activation= tf.nn.relu, input_dim=2))\n",
    "model.add(tf.keras.layers.Dropout(0.5))\n",
    "model.add(tf.keras.layers.Dense(units=128, activation= tf.nn.relu))\n",
    "model.add(tf.keras.layers.Dropout(0.5))\n",
    "model.add(tf.keras.layers.Dense(units=256, activation= tf.nn.relu))\n",
    "#model.add(tf.keras.layers.Dropout(0.5))\n",
    "model.add(tf.keras.layers.Dense(units=3, activation= tf.nn.softmax))\n",
    "model.compile(loss=tf.losses.sparse_softmax_cross_entropy, optimizer=tf.keras.optimizers.Adam(0.0001), metrics=['accuracy'])\n",
    "history = model.fit(trainingData,trainingLabel, validation_data = (testingData, testingLabel), epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs_range = range(epochs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137/137 [==============================] - 0s 57us/sample - loss: 0.6847 - acc: 0.9635\n"
     ]
    }
   ],
   "source": [
    "evlauteModel = model.evaluate(testingData, testingLabel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pleaes note: As you increae layers and number of Neurons used in each layers we get better accuracy ; in this case it's 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"plt.figure(figsize=(6,6))\\nplt.subplot(1, 2, 1)\\nplt.plot(epochs_range, acc, label='Training Accuracy')\\nplt.plot(epochs_range, val_acc, label='Validation Accuracy')\\nplt.legend(loc='lower right')\\nplt.title('Training and Validation Accuracy')\\n\\nplt.subplot(1, 2, 2)\\nplt.plot(epochs_range, loss, label='Training Loss')\\nplt.plot(epochs_range, val_loss, label='Validation Loss')\\nplt.legend(loc='upper right')\\nplt.title('Training and Validation Loss')\\nplt.savefig('./foo.png')\\nplt.show()\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Commented for testing ; uncomment section for final execution\n",
    "\"\"\"plt.figure(figsize=(6,6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.savefig('./foo.png')\n",
    "plt.show()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = 3\n",
    "prediction = model.predict(testingData)\n",
    "\n",
    "prediction = np.argmax(prediction, axis=1)\n",
    "cm = confusion_matrix(testingLabel,prediction)\n",
    "\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "    # Commented for testing ; uncomment section for final execution\n",
    "    \"\"\"plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build confusion  metrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix, without normalization\n",
      "[[115   0   0]\n",
      " [ 12   0   0]\n",
      " [  5   0   5]]\n"
     ]
    }
   ],
   "source": [
    "cm_plot_labels = [\"R\",\"G\",\"B\"]\n",
    "plot_confusion_matrix(cm, cm_plot_labels, title='Confusion Matrix')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Build new attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = Data['X1'] # extracting first col from data set\n",
    "X2 = Data['X2'] # extracting second col from data set\n",
    "X3 = []\n",
    "X4 = []\n",
    "X5 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(X1)):\n",
    "    X3.append(X1[i]*X2[i])\n",
    "    X4.append(X2[i])\n",
    "    X5.append(X1[i]*X2[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "newDataSet = Data[['X1','X2']]\n",
    "newDataSet['X3'] = X3\n",
    "newDataSet['X4'] = X4\n",
    "newDataSet['X5'] = X5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16.263398</td>\n",
       "      <td>13.299206</td>\n",
       "      <td>216.290282</td>\n",
       "      <td>13.299206</td>\n",
       "      <td>216.290282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.775408</td>\n",
       "      <td>23.986692</td>\n",
       "      <td>18.599466</td>\n",
       "      <td>23.986692</td>\n",
       "      <td>18.599466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>29.170503</td>\n",
       "      <td>-3.287474</td>\n",
       "      <td>-95.897279</td>\n",
       "      <td>-3.287474</td>\n",
       "      <td>-95.897279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6.739044</td>\n",
       "      <td>-28.033329</td>\n",
       "      <td>-188.917824</td>\n",
       "      <td>-28.033329</td>\n",
       "      <td>-188.917824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.216100</td>\n",
       "      <td>22.013695</td>\n",
       "      <td>70.798239</td>\n",
       "      <td>22.013695</td>\n",
       "      <td>70.798239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>47.374906</td>\n",
       "      <td>7.925541</td>\n",
       "      <td>375.471747</td>\n",
       "      <td>7.925541</td>\n",
       "      <td>375.471747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>24.064604</td>\n",
       "      <td>14.726109</td>\n",
       "      <td>354.377985</td>\n",
       "      <td>14.726109</td>\n",
       "      <td>354.377985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>12.667897</td>\n",
       "      <td>-16.456068</td>\n",
       "      <td>-208.463776</td>\n",
       "      <td>-16.456068</td>\n",
       "      <td>-208.463776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4.194880</td>\n",
       "      <td>20.421886</td>\n",
       "      <td>85.667357</td>\n",
       "      <td>20.421886</td>\n",
       "      <td>85.667357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>48.451924</td>\n",
       "      <td>5.783617</td>\n",
       "      <td>280.227379</td>\n",
       "      <td>5.783617</td>\n",
       "      <td>280.227379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>20.390478</td>\n",
       "      <td>-0.429726</td>\n",
       "      <td>-8.762324</td>\n",
       "      <td>-0.429726</td>\n",
       "      <td>-8.762324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-9.861540</td>\n",
       "      <td>23.623534</td>\n",
       "      <td>-232.964435</td>\n",
       "      <td>23.623534</td>\n",
       "      <td>-232.964435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.467438</td>\n",
       "      <td>24.894588</td>\n",
       "      <td>11.636676</td>\n",
       "      <td>24.894588</td>\n",
       "      <td>11.636676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-7.275756</td>\n",
       "      <td>-56.286623</td>\n",
       "      <td>409.527763</td>\n",
       "      <td>-56.286623</td>\n",
       "      <td>409.527763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-14.101633</td>\n",
       "      <td>14.115040</td>\n",
       "      <td>-199.045107</td>\n",
       "      <td>14.115040</td>\n",
       "      <td>-199.045107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>10.754342</td>\n",
       "      <td>23.666236</td>\n",
       "      <td>254.514791</td>\n",
       "      <td>23.666236</td>\n",
       "      <td>254.514791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-15.481186</td>\n",
       "      <td>-23.009161</td>\n",
       "      <td>356.209116</td>\n",
       "      <td>-23.009161</td>\n",
       "      <td>356.209116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-1.220046</td>\n",
       "      <td>2.465586</td>\n",
       "      <td>-3.008129</td>\n",
       "      <td>2.465586</td>\n",
       "      <td>-3.008129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>17.800206</td>\n",
       "      <td>15.209416</td>\n",
       "      <td>270.730730</td>\n",
       "      <td>15.209416</td>\n",
       "      <td>270.730730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-22.414667</td>\n",
       "      <td>-5.052174</td>\n",
       "      <td>113.242796</td>\n",
       "      <td>-5.052174</td>\n",
       "      <td>113.242796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>-24.190381</td>\n",
       "      <td>-3.003693</td>\n",
       "      <td>72.660475</td>\n",
       "      <td>-3.003693</td>\n",
       "      <td>72.660475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>-22.877052</td>\n",
       "      <td>4.274241</td>\n",
       "      <td>-97.782040</td>\n",
       "      <td>4.274241</td>\n",
       "      <td>-97.782040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>25.051359</td>\n",
       "      <td>2.531966</td>\n",
       "      <td>63.429196</td>\n",
       "      <td>2.531966</td>\n",
       "      <td>63.429196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>3.091185</td>\n",
       "      <td>-24.198205</td>\n",
       "      <td>-74.801140</td>\n",
       "      <td>-24.198205</td>\n",
       "      <td>-74.801140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>5.997345</td>\n",
       "      <td>-27.517897</td>\n",
       "      <td>-165.034318</td>\n",
       "      <td>-27.517897</td>\n",
       "      <td>-165.034318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>-20.062580</td>\n",
       "      <td>-11.521785</td>\n",
       "      <td>231.156720</td>\n",
       "      <td>-11.521785</td>\n",
       "      <td>231.156720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>-5.370937</td>\n",
       "      <td>21.898004</td>\n",
       "      <td>-117.612794</td>\n",
       "      <td>21.898004</td>\n",
       "      <td>-117.612794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>19.113578</td>\n",
       "      <td>-10.048873</td>\n",
       "      <td>-192.069911</td>\n",
       "      <td>-10.048873</td>\n",
       "      <td>-192.069911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>18.405937</td>\n",
       "      <td>7.929500</td>\n",
       "      <td>145.949876</td>\n",
       "      <td>7.929500</td>\n",
       "      <td>145.949876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>37.025255</td>\n",
       "      <td>26.260656</td>\n",
       "      <td>972.307478</td>\n",
       "      <td>26.260656</td>\n",
       "      <td>972.307478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>655</th>\n",
       "      <td>16.013990</td>\n",
       "      <td>15.469119</td>\n",
       "      <td>247.722323</td>\n",
       "      <td>15.469119</td>\n",
       "      <td>247.722323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>656</th>\n",
       "      <td>-9.534431</td>\n",
       "      <td>20.163994</td>\n",
       "      <td>-192.252218</td>\n",
       "      <td>20.163994</td>\n",
       "      <td>-192.252218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>657</th>\n",
       "      <td>8.919776</td>\n",
       "      <td>24.863858</td>\n",
       "      <td>221.780036</td>\n",
       "      <td>24.863858</td>\n",
       "      <td>221.780036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>658</th>\n",
       "      <td>-7.603234</td>\n",
       "      <td>-25.878660</td>\n",
       "      <td>196.761493</td>\n",
       "      <td>-25.878660</td>\n",
       "      <td>196.761493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>659</th>\n",
       "      <td>53.204253</td>\n",
       "      <td>4.196494</td>\n",
       "      <td>223.271330</td>\n",
       "      <td>4.196494</td>\n",
       "      <td>223.271330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>660</th>\n",
       "      <td>-15.556501</td>\n",
       "      <td>16.456314</td>\n",
       "      <td>-256.002652</td>\n",
       "      <td>16.456314</td>\n",
       "      <td>-256.002652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>661</th>\n",
       "      <td>-25.991651</td>\n",
       "      <td>4.336594</td>\n",
       "      <td>-112.715241</td>\n",
       "      <td>4.336594</td>\n",
       "      <td>-112.715241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>662</th>\n",
       "      <td>-24.741184</td>\n",
       "      <td>-10.836943</td>\n",
       "      <td>268.118818</td>\n",
       "      <td>-10.836943</td>\n",
       "      <td>268.118818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>663</th>\n",
       "      <td>37.033995</td>\n",
       "      <td>-21.082265</td>\n",
       "      <td>-780.760509</td>\n",
       "      <td>-21.082265</td>\n",
       "      <td>-780.760509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>664</th>\n",
       "      <td>-27.245006</td>\n",
       "      <td>3.467136</td>\n",
       "      <td>-94.462144</td>\n",
       "      <td>3.467136</td>\n",
       "      <td>-94.462144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>665</th>\n",
       "      <td>-12.178696</td>\n",
       "      <td>23.480744</td>\n",
       "      <td>-285.964856</td>\n",
       "      <td>23.480744</td>\n",
       "      <td>-285.964856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>666</th>\n",
       "      <td>-12.185042</td>\n",
       "      <td>25.241583</td>\n",
       "      <td>-307.569744</td>\n",
       "      <td>25.241583</td>\n",
       "      <td>-307.569744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>667</th>\n",
       "      <td>-9.166683</td>\n",
       "      <td>22.334445</td>\n",
       "      <td>-204.732780</td>\n",
       "      <td>22.334445</td>\n",
       "      <td>-204.732780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>668</th>\n",
       "      <td>-18.910669</td>\n",
       "      <td>-9.802342</td>\n",
       "      <td>185.368849</td>\n",
       "      <td>-9.802342</td>\n",
       "      <td>185.368849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>669</th>\n",
       "      <td>-9.222427</td>\n",
       "      <td>19.401378</td>\n",
       "      <td>-178.927786</td>\n",
       "      <td>19.401378</td>\n",
       "      <td>-178.927786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>670</th>\n",
       "      <td>27.313293</td>\n",
       "      <td>8.892050</td>\n",
       "      <td>242.871177</td>\n",
       "      <td>8.892050</td>\n",
       "      <td>242.871177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>671</th>\n",
       "      <td>-14.629075</td>\n",
       "      <td>60.321524</td>\n",
       "      <td>-882.448097</td>\n",
       "      <td>60.321524</td>\n",
       "      <td>-882.448097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>672</th>\n",
       "      <td>-16.811583</td>\n",
       "      <td>-17.629605</td>\n",
       "      <td>296.381553</td>\n",
       "      <td>-17.629605</td>\n",
       "      <td>296.381553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>673</th>\n",
       "      <td>25.887799</td>\n",
       "      <td>55.202370</td>\n",
       "      <td>1429.067865</td>\n",
       "      <td>55.202370</td>\n",
       "      <td>1429.067865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>674</th>\n",
       "      <td>-30.917686</td>\n",
       "      <td>2.650866</td>\n",
       "      <td>-81.958659</td>\n",
       "      <td>2.650866</td>\n",
       "      <td>-81.958659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>675</th>\n",
       "      <td>-1.504464</td>\n",
       "      <td>-28.267912</td>\n",
       "      <td>42.528056</td>\n",
       "      <td>-28.267912</td>\n",
       "      <td>42.528056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>676</th>\n",
       "      <td>20.100676</td>\n",
       "      <td>16.314787</td>\n",
       "      <td>327.938240</td>\n",
       "      <td>16.314787</td>\n",
       "      <td>327.938240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>677</th>\n",
       "      <td>4.556231</td>\n",
       "      <td>-0.101599</td>\n",
       "      <td>-0.462908</td>\n",
       "      <td>-0.101599</td>\n",
       "      <td>-0.462908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>678</th>\n",
       "      <td>-27.331002</td>\n",
       "      <td>-0.250465</td>\n",
       "      <td>6.845458</td>\n",
       "      <td>-0.250465</td>\n",
       "      <td>6.845458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>679</th>\n",
       "      <td>-7.705980</td>\n",
       "      <td>-19.082421</td>\n",
       "      <td>147.048745</td>\n",
       "      <td>-19.082421</td>\n",
       "      <td>147.048745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>680</th>\n",
       "      <td>0.244329</td>\n",
       "      <td>19.916511</td>\n",
       "      <td>4.866189</td>\n",
       "      <td>19.916511</td>\n",
       "      <td>4.866189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>681</th>\n",
       "      <td>22.591032</td>\n",
       "      <td>-9.133996</td>\n",
       "      <td>-206.346394</td>\n",
       "      <td>-9.133996</td>\n",
       "      <td>-206.346394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>682</th>\n",
       "      <td>-21.533493</td>\n",
       "      <td>-7.396731</td>\n",
       "      <td>159.277458</td>\n",
       "      <td>-7.396731</td>\n",
       "      <td>159.277458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>683</th>\n",
       "      <td>5.699510</td>\n",
       "      <td>30.742204</td>\n",
       "      <td>175.215505</td>\n",
       "      <td>30.742204</td>\n",
       "      <td>175.215505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>684</th>\n",
       "      <td>11.732251</td>\n",
       "      <td>19.427124</td>\n",
       "      <td>227.923891</td>\n",
       "      <td>19.427124</td>\n",
       "      <td>227.923891</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>685 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            X1         X2           X3         X4           X5\n",
       "0    16.263398  13.299206   216.290282  13.299206   216.290282\n",
       "1     0.775408  23.986692    18.599466  23.986692    18.599466\n",
       "2    29.170503  -3.287474   -95.897279  -3.287474   -95.897279\n",
       "3     6.739044 -28.033329  -188.917824 -28.033329  -188.917824\n",
       "4     3.216100  22.013695    70.798239  22.013695    70.798239\n",
       "5    47.374906   7.925541   375.471747   7.925541   375.471747\n",
       "6    24.064604  14.726109   354.377985  14.726109   354.377985\n",
       "7    12.667897 -16.456068  -208.463776 -16.456068  -208.463776\n",
       "8     4.194880  20.421886    85.667357  20.421886    85.667357\n",
       "9    48.451924   5.783617   280.227379   5.783617   280.227379\n",
       "10   20.390478  -0.429726    -8.762324  -0.429726    -8.762324\n",
       "11   -9.861540  23.623534  -232.964435  23.623534  -232.964435\n",
       "12    0.467438  24.894588    11.636676  24.894588    11.636676\n",
       "13   -7.275756 -56.286623   409.527763 -56.286623   409.527763\n",
       "14  -14.101633  14.115040  -199.045107  14.115040  -199.045107\n",
       "15   10.754342  23.666236   254.514791  23.666236   254.514791\n",
       "16  -15.481186 -23.009161   356.209116 -23.009161   356.209116\n",
       "17   -1.220046   2.465586    -3.008129   2.465586    -3.008129\n",
       "18   17.800206  15.209416   270.730730  15.209416   270.730730\n",
       "19  -22.414667  -5.052174   113.242796  -5.052174   113.242796\n",
       "20  -24.190381  -3.003693    72.660475  -3.003693    72.660475\n",
       "21  -22.877052   4.274241   -97.782040   4.274241   -97.782040\n",
       "22   25.051359   2.531966    63.429196   2.531966    63.429196\n",
       "23    3.091185 -24.198205   -74.801140 -24.198205   -74.801140\n",
       "24    5.997345 -27.517897  -165.034318 -27.517897  -165.034318\n",
       "25  -20.062580 -11.521785   231.156720 -11.521785   231.156720\n",
       "26   -5.370937  21.898004  -117.612794  21.898004  -117.612794\n",
       "27   19.113578 -10.048873  -192.069911 -10.048873  -192.069911\n",
       "28   18.405937   7.929500   145.949876   7.929500   145.949876\n",
       "29   37.025255  26.260656   972.307478  26.260656   972.307478\n",
       "..         ...        ...          ...        ...          ...\n",
       "655  16.013990  15.469119   247.722323  15.469119   247.722323\n",
       "656  -9.534431  20.163994  -192.252218  20.163994  -192.252218\n",
       "657   8.919776  24.863858   221.780036  24.863858   221.780036\n",
       "658  -7.603234 -25.878660   196.761493 -25.878660   196.761493\n",
       "659  53.204253   4.196494   223.271330   4.196494   223.271330\n",
       "660 -15.556501  16.456314  -256.002652  16.456314  -256.002652\n",
       "661 -25.991651   4.336594  -112.715241   4.336594  -112.715241\n",
       "662 -24.741184 -10.836943   268.118818 -10.836943   268.118818\n",
       "663  37.033995 -21.082265  -780.760509 -21.082265  -780.760509\n",
       "664 -27.245006   3.467136   -94.462144   3.467136   -94.462144\n",
       "665 -12.178696  23.480744  -285.964856  23.480744  -285.964856\n",
       "666 -12.185042  25.241583  -307.569744  25.241583  -307.569744\n",
       "667  -9.166683  22.334445  -204.732780  22.334445  -204.732780\n",
       "668 -18.910669  -9.802342   185.368849  -9.802342   185.368849\n",
       "669  -9.222427  19.401378  -178.927786  19.401378  -178.927786\n",
       "670  27.313293   8.892050   242.871177   8.892050   242.871177\n",
       "671 -14.629075  60.321524  -882.448097  60.321524  -882.448097\n",
       "672 -16.811583 -17.629605   296.381553 -17.629605   296.381553\n",
       "673  25.887799  55.202370  1429.067865  55.202370  1429.067865\n",
       "674 -30.917686   2.650866   -81.958659   2.650866   -81.958659\n",
       "675  -1.504464 -28.267912    42.528056 -28.267912    42.528056\n",
       "676  20.100676  16.314787   327.938240  16.314787   327.938240\n",
       "677   4.556231  -0.101599    -0.462908  -0.101599    -0.462908\n",
       "678 -27.331002  -0.250465     6.845458  -0.250465     6.845458\n",
       "679  -7.705980 -19.082421   147.048745 -19.082421   147.048745\n",
       "680   0.244329  19.916511     4.866189  19.916511     4.866189\n",
       "681  22.591032  -9.133996  -206.346394  -9.133996  -206.346394\n",
       "682 -21.533493  -7.396731   159.277458  -7.396731   159.277458\n",
       "683   5.699510  30.742204   175.215505  30.742204   175.215505\n",
       "684  11.732251  19.427124   227.923891  19.427124   227.923891\n",
       "\n",
       "[685 rows x 5 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newDataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
